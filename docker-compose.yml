# Docker Compose file for Pipeline X (Production Ready)

x-airflow-common:
  &airflow-common
  build: .
  image: pipeline-x-airflow:latest
  env_file:
    - .env
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    # Note: If running Hybrid, these variables pull from .env automatically
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    PYTHONPATH: /opt/airflow
    QDRANT_HOST: ${QDRANT_HOST}
    QDRANT_PORT: ${QDRANT_PORT}
    API_URL: ${API_URL}
    # Azure Credentials for Airflow/Spark
    AZURE_STORAGE_ACCOUNT_NAME: ${AZURE_STORAGE_ACCOUNT_NAME}
    AZURE_STORAGE_ACCOUNT_KEY: ${AZURE_STORAGE_ACCOUNT_KEY}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/src
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy
    qdrant:
      condition: service_started

services:
  # ---------------------------------------------------------
  # 1. DATABASE LAYER
  # ---------------------------------------------------------
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 10s
      retries: 5
      start_period: 10s
    ports:
      - "5432:5432"

  # ---------------------------------------------------------
  # 2. VECTOR ENGINE (Qdrant)
  # ---------------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: pipeline-x-qdrant-1
    ports:
      - "6333:6333"
    volumes:
      - qdrant-storage:/qdrant/storage

  # ---------------------------------------------------------
  # 3. AIRFLOW INITIALIZATION
  # ---------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    container_name: pipeline-x-airflow-init
    entrypoint: /bin/bash
    command: >
      -c "airflow db init && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin"
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'

  # ---------------------------------------------------------
  # 4. AIRFLOW SERVICES
  # ---------------------------------------------------------
  airflow-webserver:
    <<: *airflow-common
    container_name: pipeline-x-airflow-webserver-1
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: pipeline-x-airflow-scheduler-1
    command: scheduler
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  # ---------------------------------------------------------
  # 5. RAG SERVICES (API + UI)
  # ---------------------------------------------------------
  api:
    <<: *airflow-common
    container_name: pipeline-x-api-1
    entrypoint: /bin/bash
    # Ensure it listens on 0.0.0.0 so UI can find it
    command: >
      -c "uvicorn src.api.main:app --host 0.0.0.0 --port 8000"
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      retries: 5
      start_period: 20s
    depends_on:
      qdrant:
        condition: service_started

  ui:
    <<: *airflow-common
    container_name: pipeline-x-ui-1
    entrypoint: /bin/bash
    # FIX: Use Absolute Path (/opt/airflow/src/...) to avoid "File does not exist" error
    command: >
      -c "export PYTHONPATH=$PYTHONPATH:/opt/airflow && streamlit run /opt/airflow/src/ui/app.py --server.port 8501 --server.address 0.0.0.0"
    ports:
      - "8501:8501"
    depends_on:
      api:
        condition: service_healthy
  
  # ---------------------------------------------------------
  # 6. BIG DATA ENGINE (Spark)
  # ---------------------------------------------------------
  spark-master:
    image: apache/spark:3.5.1
    container_name: pipeline-x-spark-master-1
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.master.Master
    ports:
      - "8081:8080"
      - "7077:7077"

  spark-worker:
    image: apache/spark:3.5.1
    container_name: pipeline-x-spark-worker-1
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    depends_on:
      - spark-master

volumes:
  postgres-db-volume:
  qdrant-storage: